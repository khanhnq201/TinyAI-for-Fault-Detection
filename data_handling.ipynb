{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3c73711",
   "metadata": {},
   "source": [
    "Các lưu ý: \n",
    " - Set random state để có thể tái tạo kết quả \n",
    " - Chia file thành các phần nhỏ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a824f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (7822, 1, 32, 32)\n",
      "Original labels shape: (7822,)\n",
      "==================================================\n",
      "Original Data Distribution:\n",
      "Class 0: 1420 samples (18.15%)\n",
      "Class 1: 711 samples (9.09%)\n",
      "Class 2: 711 samples (9.09%)\n",
      "Class 3: 711 samples (9.09%)\n",
      "Class 4: 711 samples (9.09%)\n",
      "Class 5: 708 samples (9.05%)\n",
      "Class 6: 711 samples (9.09%)\n",
      "Class 7: 714 samples (9.13%)\n",
      "Class 8: 711 samples (9.09%)\n",
      "Class 9: 714 samples (9.13%)\n",
      "==================================================\n",
      "Final split results:\n",
      "Test set - Total samples: 2120\n",
      "  Class 0: 212 samples\n",
      "  Class 1: 212 samples\n",
      "  Class 2: 212 samples\n",
      "  Class 3: 212 samples\n",
      "  Class 4: 212 samples\n",
      "  Class 5: 212 samples\n",
      "  Class 6: 212 samples\n",
      "  Class 7: 212 samples\n",
      "  Class 8: 212 samples\n",
      "  Class 9: 212 samples\n",
      "\n",
      "Train set - Total samples: 1250\n",
      "  Class 0: 800 samples\n",
      "  Class 1: 50 samples\n",
      "  Class 2: 50 samples\n",
      "  Class 3: 50 samples\n",
      "  Class 4: 50 samples\n",
      "  Class 5: 50 samples\n",
      "  Class 6: 50 samples\n",
      "  Class 7: 50 samples\n",
      "  Class 8: 50 samples\n",
      "  Class 9: 50 samples\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "import scipy.io\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "from torchinfo import summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "SAMPLE_LENGTH = 1024\n",
    "\n",
    "SENSOR = 'all'\n",
    "LOAD_LEVELS = [0,1]\n",
    "FAULT_TYPE = list(range(10))\n",
    "NUM_CLASSES = len(FAULT_TYPE)\n",
    "\n",
    "SIZE_INPUT = int(SAMPLE_LENGTH**(1/2))\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "NORMAL_SAMPLES_USED = 800\n",
    "CLASS_RATIOS = {0: 16, \n",
    "        1: 1,\n",
    "        2: 1,\n",
    "        3: 1,\n",
    "        4: 1,\n",
    "        5: 1,\n",
    "        6: 1,\n",
    "        7: 1,\n",
    "        8: 1,\n",
    "        9: 1}\n",
    "\n",
    "# DATA HANDLING\n",
    "def import_file(bearing = 'DE', fault_type = 0, load_level = 0, sensor = 'all', print_infor = False, sample_length = 1024, base_path = 'CWRU-dataset-main'):\n",
    "    \"\"\"\n",
    "    Import data \n",
    "\n",
    "    Args:\n",
    "        bearing (str): 'DE' or 'FE'\n",
    "        fault_type (int): label from 0 to 9 \n",
    "        load_level (int): 0 HP, 1 HP, 2 HP\n",
    "        sensor (str) : 'DE', 'FE', 'BA', 'all'\n",
    "        base_path (str): Base path to dataset\n",
    "\n",
    "        output: data (numpy array)\n",
    "    \"\"\"\n",
    "    if bearing == 'DE': \n",
    "        base_data_directory = os.path.join(base_path, '12k_Drive_End_Bearing_Fault_Data/')\n",
    "    else: \n",
    "        base_data_directory = os.path.join(base_path, '12k_Fan_End_Bearing_Fault_Data/')\n",
    "\n",
    "    fault_dict = {\n",
    "        0 : 'Normal/',\n",
    "        1 : 'B/007/',\n",
    "        2 : 'B/014/',\n",
    "        3 : 'B/021/',\n",
    "        4 : 'IR/007/',\n",
    "        5 : 'IR/014/',\n",
    "        6 : 'IR/021/',\n",
    "        7 : 'OR/007/@6/',\n",
    "        8 : 'OR/014/',\n",
    "        9 : 'OR/021/@6/'\n",
    "    }\n",
    "    load_level_dict = {\n",
    "        0 : '_0',\n",
    "        1 : '_1',\n",
    "        2 : '_2',\n",
    "    }\n",
    "\n",
    "    file_path = os.path.join(base_data_directory, fault_dict[fault_type])\n",
    "    full_file_path = None\n",
    "\n",
    "    try:\n",
    "        file_list = os.listdir(file_path)\n",
    "        for file in file_list: \n",
    "            full_path = os.path.join(file_path, file)\n",
    "            if os.path.isfile(full_path): \n",
    "                if load_level_dict[load_level] in file: \n",
    "                    full_file_path = full_path\n",
    "\n",
    "            if full_file_path:\n",
    "                break\n",
    "    except Exception as e: \n",
    "        print(f'File path not exists: {e}')\n",
    "        return np.array([])\n",
    "\n",
    "    if full_file_path is None:\n",
    "        print(f'No file found for bearing={bearing}, fault_type={fault_type}, load_level={load_level}')\n",
    "        return np.array([])\n",
    "\n",
    "    data = np.array([])\n",
    "    data_import = None\n",
    "    mat_data = scipy.io.loadmat(full_file_path)\n",
    "    keys_in_file = list(mat_data.keys())\n",
    "\n",
    "    for key in keys_in_file: \n",
    "        if sensor == 'all' and ('DE' in key or 'FE' in key or 'BA' in key):\n",
    "            data_import = mat_data[key].flatten()\n",
    "            length = (len(data_import) // sample_length) * sample_length\n",
    "            data_import = data_import[:length]\n",
    "            data = np.append(data, data_import)\n",
    "            if print_infor: \n",
    "                print('================= Import data =====================')\n",
    "                print(f' - Process file: {full_file_path}')\n",
    "                print(f' - Key: {key}')\n",
    "                print(f' - Data length: {len(data)/sample_length}')\n",
    "                print('===================================================')\n",
    "        elif sensor in key:\n",
    "            data = mat_data[key].flatten()\n",
    "            length = (len(data)//sample_length) * sample_length\n",
    "            data = data[:length]\n",
    "            if print_infor: \n",
    "                print('================= Import data =====================')\n",
    "                print(f' - Process file: {full_file_path}')\n",
    "                print(f' - Key: {key}')\n",
    "                print(f' - Data length: {len(data)/sample_length}')\n",
    "                print('===================================================')\n",
    "            break\n",
    "\n",
    "    return data\n",
    "\n",
    "def import_data(bearing = 'DE', load_level = list(range(3)), fault_type=list(range(10)), sensor = 'all', sample_length = 1024, base_path = 'CWRU-dataset-main'):\n",
    "    \"\"\"Enhanced import_data with better structure\"\"\"\n",
    "    X_data = []\n",
    "    Y_data = []\n",
    "\n",
    "    for load in load_level: \n",
    "        for fault_type_label in fault_type:\n",
    "            data = import_file(bearing, fault_type_label, load, sensor, False, sample_length, base_path)\n",
    "            if len(data) > 0:\n",
    "                data_reshaped = data.reshape(-1, sample_length)\n",
    "                labels = np.full(data_reshaped.shape[0], fault_type_label)\n",
    "                X_data.append(data_reshaped)\n",
    "                Y_data.append(labels)\n",
    "\n",
    "    if not X_data:\n",
    "        raise ValueError(\"No data was loaded. Please check your parameters and file paths.\")\n",
    "    \n",
    "    final_X_data = np.concatenate(X_data, axis=0)\n",
    "    final_Y_data = np.concatenate(Y_data, axis=0)\n",
    "\n",
    "    final_X_data = np.reshape(final_X_data, (-1, 1, SIZE_INPUT, SIZE_INPUT))\n",
    "    \n",
    "    print('Original data shape:', final_X_data.shape)\n",
    "    print('Original labels shape:', final_Y_data.shape)\n",
    "\n",
    "    return final_X_data, final_Y_data\n",
    "\n",
    "def create_imbalanced_split(X_data, Y_data, normal_samples=None, class_ratios=None, test_size=0.3, random_state=42):\n",
    "    \"\"\"\n",
    "    Create train/test split for imbalanced data\n",
    "    \n",
    "    Args:\n",
    "        X_data: Input features\n",
    "        Y_data: Labels\n",
    "        normal_samples: Number of normal class samples to use (None = use all)\n",
    "        class_ratios: Dictionary defining ratio between classes \n",
    "                     e.g., {0: 4, 4: 1, 1: 2, 7: 1} means normal:IR007:B007:OR007 = 4:1:2:1\n",
    "        test_size: Proportion for test set (default 0.3)\n",
    "        random_state: Random seed\n",
    "    \n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get class distribution\n",
    "    unique_classes, class_counts = np.unique(Y_data, return_counts=True)\n",
    "\n",
    "    # Step 1: Create balanced test set\n",
    "    min_class_samples = min(class_counts)\n",
    "    test_samples_per_class = int(min_class_samples * test_size)\n",
    "    \n",
    "    X_test_list = []\n",
    "    y_test_list = []\n",
    "    X_train_temp_list = []\n",
    "    y_train_temp_list = []\n",
    "    \n",
    "    # Split each class separately to ensure balanced test set\n",
    "    for cls in unique_classes:\n",
    "        cls_indices = np.where(Y_data == cls)[0]\n",
    "        X_cls = X_data[cls_indices]\n",
    "        y_cls = Y_data[cls_indices]\n",
    "        \n",
    "        # Random split for this class\n",
    "        X_cls_train, X_cls_test, y_cls_train, y_cls_test = train_test_split(\n",
    "            X_cls, y_cls, test_size=test_samples_per_class, \n",
    "            random_state=random_state, stratify=None\n",
    "        )\n",
    "        \n",
    "        X_test_list.append(X_cls_test)\n",
    "        y_test_list.append(y_cls_test)\n",
    "        X_train_temp_list.append(X_cls_train)\n",
    "        y_train_temp_list.append(y_cls_train)\n",
    "    \n",
    "    # Combine test data\n",
    "    X_test = np.concatenate(X_test_list, axis=0)\n",
    "    y_test = np.concatenate(y_test_list, axis=0)\n",
    "    \n",
    "    # Combine remaining training data\n",
    "    X_train_temp = np.concatenate(X_train_temp_list, axis=0)\n",
    "    y_train_temp = np.concatenate(y_train_temp_list, axis=0)\n",
    "    \n",
    "    # Step 2: Create imbalanced training set based on user specifications\n",
    "    if class_ratios is None:\n",
    "        # If no ratios specified, use all remaining training data\n",
    "        X_train = X_train_temp\n",
    "        y_train = y_train_temp\n",
    "    else:\n",
    "        # Apply class ratios to create imbalanced training set\n",
    "        X_train_list = []\n",
    "        y_train_list = []\n",
    "        \n",
    "        # Determine number of samples for each class based on ratios\n",
    "        normal_class = 0  # Assuming class 0 is normal\n",
    "        \n",
    "        if normal_samples is None:\n",
    "            # Use all available normal samples\n",
    "            normal_indices = np.where(y_train_temp == normal_class)[0]\n",
    "            normal_samples = len(normal_indices)\n",
    "        \n",
    "        # Calculate samples for each class based on ratios\n",
    "        normal_ratio = class_ratios.get(normal_class, 1)\n",
    "        \n",
    "        # First pass: determine actual normal samples to use as base\n",
    "        normal_indices = np.where(y_train_temp == normal_class)[0]\n",
    "        available_normal = len(normal_indices)\n",
    "        actual_normal_samples = min(normal_samples, available_normal)\n",
    "        \n",
    "        for cls in unique_classes:\n",
    "            cls_indices = np.where(y_train_temp == cls)[0]\n",
    "            available_samples = len(cls_indices)\n",
    "            \n",
    "            if cls == normal_class:\n",
    "                # Use the determined number of normal samples\n",
    "                target_samples = actual_normal_samples\n",
    "            else:\n",
    "                # Calculate based on ratio to normal class\n",
    "                cls_ratio = class_ratios.get(cls, 0)  # Default to 0 if class not in ratios\n",
    "                if cls_ratio > 0:\n",
    "                    target_samples = int((actual_normal_samples * cls_ratio) / normal_ratio)\n",
    "                    target_samples = min(target_samples, available_samples)\n",
    "                else:\n",
    "                    target_samples = 0  # Skip classes not in ratios\n",
    "            \n",
    "            # Randomly sample the target number\n",
    "            if target_samples > 0:\n",
    "                selected_indices = np.random.choice(cls_indices, target_samples, replace=False)\n",
    "                X_train_list.append(X_train_temp[selected_indices])\n",
    "                y_train_list.append(y_train_temp[selected_indices])\n",
    "        \n",
    "        X_train = np.concatenate(X_train_list, axis=0)\n",
    "        y_train = np.concatenate(y_train_list, axis=0)\n",
    "    \n",
    "    # Shuffle training data\n",
    "    train_indices = np.random.permutation(len(X_train))\n",
    "    X_train = X_train[train_indices]\n",
    "    y_train = y_train[train_indices]\n",
    "    \n",
    "    # Shuffle test data\n",
    "    test_indices = np.random.permutation(len(X_test))\n",
    "    X_test = X_test[test_indices]\n",
    "    y_test = y_test[test_indices]\n",
    "    \n",
    "    # Print final distribution\n",
    "    print('='*50)\n",
    "    print(f\"Final split results:\")\n",
    "    print(f\"Test set - Total samples: {len(y_test)}\")\n",
    "    test_unique, test_counts = np.unique(y_test, return_counts=True)\n",
    "    for cls, count in zip(test_unique, test_counts):\n",
    "        print(f\"  Class {cls}: {count} samples\")\n",
    "    \n",
    "    print(f\"\\nTrain set - Total samples: {len(y_train)}\")\n",
    "    train_unique, train_counts = np.unique(y_train, return_counts=True)\n",
    "    for cls, count in zip(train_unique, train_counts):\n",
    "        print(f\"  Class {cls}: {count} samples\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def analyze_class_distribution(y_data, title=\"Class Distribution\"):\n",
    "    \"\"\"Analyze and visualize class distribution\"\"\"\n",
    "    unique_classes, counts = np.unique(y_data, return_counts=True)\n",
    "    \n",
    "    print('='*50)\n",
    "    print(f\"{title}:\")\n",
    "    for cls, count in zip(unique_classes, counts):\n",
    "        percentage = (count / len(y_data)) * 100\n",
    "        print(f\"Class {cls}: {count} samples ({percentage:.2f}%)\")\n",
    "    \n",
    "    return dict(zip(unique_classes, counts))\n",
    "\n",
    "# Load data\n",
    "X_data, Y_data = import_data(load_level=LOAD_LEVELS, fault_type=FAULT_TYPE, sensor=SENSOR, sample_length=SAMPLE_LENGTH)\n",
    "\n",
    "# Analyze original distribution\n",
    "analyze_class_distribution(Y_data, \"Original Data Distribution\")\n",
    "\n",
    "# Create_imbalanced_split\n",
    "X_train, X_test, y_train, y_test = create_imbalanced_split(\n",
    "    X_data, Y_data, \n",
    "    normal_samples=NORMAL_SAMPLES_USED,  # Use only 500 normal samples\n",
    "    class_ratios=CLASS_RATIOS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce5761d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Shape of X_train before SMOTE: (1250, 1, 32, 32)\n",
      "Shape of y_train before SMOTE: (1250,)\n",
      "==================================================\n",
      "Training Data Distribution Before SMOTE:\n",
      "Class 0: 800 samples (64.00%)\n",
      "Class 1: 50 samples (4.00%)\n",
      "Class 2: 50 samples (4.00%)\n",
      "Class 3: 50 samples (4.00%)\n",
      "Class 4: 50 samples (4.00%)\n",
      "Class 5: 50 samples (4.00%)\n",
      "Class 6: 50 samples (4.00%)\n",
      "Class 7: 50 samples (4.00%)\n",
      "Class 8: 50 samples (4.00%)\n",
      "Class 9: 50 samples (4.00%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 800, 1: 50, 2: 50, 3: 50, 4: 50, 5: 50, 6: 50, 7: 50, 8: 50, 9: 50}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "print('='*50)\n",
    "print(f\"Shape of X_train before SMOTE: {X_train.shape}\")\n",
    "print(f\"Shape of y_train before SMOTE: {y_train.shape}\")\n",
    "analyze_class_distribution(y_train, \"Training Data Distribution Before SMOTE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c549473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts before SMOTE: Counter({0: 800, 6: 50, 9: 50, 3: 50, 2: 50, 4: 50, 5: 50, 1: 50, 8: 50, 7: 50})\n",
      "Applying SMOTE with strategy: not majority\n",
      "==================================================\n",
      "Shape of X_train after SMOTE: (8000, 1024)\n",
      "Shape of y_train after SMOTE: (8000,)\n",
      "==================================================\n",
      "Training Data Distribution After SMOTE:\n",
      "Class 0: 800 samples (10.00%)\n",
      "Class 1: 800 samples (10.00%)\n",
      "Class 2: 800 samples (10.00%)\n",
      "Class 3: 800 samples (10.00%)\n",
      "Class 4: 800 samples (10.00%)\n",
      "Class 5: 800 samples (10.00%)\n",
      "Class 6: 800 samples (10.00%)\n",
      "Class 7: 800 samples (10.00%)\n",
      "Class 8: 800 samples (10.00%)\n",
      "Class 9: 800 samples (10.00%)\n"
     ]
    }
   ],
   "source": [
    "# Reshape X_train for SMOTE\n",
    "# Original shape: (n_samples, channels, height, width) e.g. (N, 1, 32, 32)\n",
    "n_samples_train, channels, height, width = X_train.shape\n",
    "X_train_reshaped = X_train.reshape(n_samples_train, channels * height * width) # (N, 1024)\n",
    "\n",
    "# Initialize SMOTE\n",
    "# Option 1: Balance all classes relative to the majority\n",
    "# smote = SMOTE(random_state=42) # Default is 'auto' which is equivalent to 'not majority'\n",
    "                                # or use sampling_strategy='all' to make all classes have same count as majority\n",
    "\n",
    "# Option 2: Specify desired number of samples per class (RECOMMENDED for control)\n",
    "# First, find out the current counts\n",
    "train_counts_before_smote = Counter(y_train)\n",
    "print(f\"Counts before SMOTE: {train_counts_before_smote}\")\n",
    "\n",
    "# Example: Upsample all minority classes (1-9) to be, say, 80% of the normal class (0)\n",
    "# or a fixed number like 500 samples each if available after create_imbalanced_split\n",
    "# This needs careful thought based on your goals and initial counts from create_imbalanced_split\n",
    "\n",
    "# Let's assume class 0 is the majority after create_imbalanced_split\n",
    "# and we want to upsample classes 1-9.\n",
    "# A common strategy is 'not majority' which upsamples all minority classes to match the majority.\n",
    "# Or, if you want finer control:\n",
    "# target_samples_minority = int(train_counts_before_smote[0] * 0.8) # e.g., 80% of normal class\n",
    "# sampling_strategy_custom = {cls: max(count, target_samples_minority) for cls, count in train_counts_before_smote.items()}\n",
    "# for cls in range(NUM_CLASSES): # Ensure all classes are in the strategy\n",
    "#    if cls not in sampling_strategy_custom:\n",
    "#        sampling_strategy_custom[cls] = train_counts_before_smote.get(cls, 0) # Keep original count if not specified or 0\n",
    "#    if cls != 0 and cls in train_counts_before_smote: # For minority classes\n",
    "#        sampling_strategy_custom[cls] = max(train_counts_before_smote[cls], target_samples_minority)\n",
    "#    elif cls == 0: # Keep majority class as is\n",
    "#        sampling_strategy_custom[cls] = train_counts_before_smote[0]\n",
    "\n",
    "# A simpler and often effective strategy: upsample all minority classes to match the majority.\n",
    "# If class 0 is the majority as per your CLASS_RATIOS, then 'not majority' will upsample 1-9.\n",
    "smote = SMOTE(sampling_strategy='not majority', random_state=42, k_neighbors=5)\n",
    "# If you want all classes to have the same number of samples as the *original* majority class:\n",
    "# smote = SMOTE(sampling_strategy='auto', random_state=42) # 'auto' is equivalent to 'not majority'\n",
    "# If you want all classes to have the same number of samples (equal to the current majority after create_imbalanced_split):\n",
    "# smote = SMOTE(sampling_strategy='all', random_state=42)\n",
    "\n",
    "print(f\"Applying SMOTE with strategy: {smote.sampling_strategy}\")\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_reshaped, y_train)\n",
    "\n",
    "print('='*50)\n",
    "print(f\"Shape of X_train after SMOTE: {X_train_smote.shape}\")\n",
    "print(f\"Shape of y_train after SMOTE: {y_train_smote.shape}\")\n",
    "analyze_class_distribution(y_train_smote, \"Training Data Distribution After SMOTE\")\n",
    "\n",
    "# Reshape X_train_smote back to original image-like format\n",
    "X_train_final = X_train_smote.reshape(-1, channels, height, width)\n",
    "\n",
    "# Now use X_train_final and y_train_smote for your DataLoader\n",
    "train_dataset = BearingDataset(X_train_final, y_train_smote)\n",
    "# ... (rest of your code for DataLoader, model training, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce487677",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BearingDataset(Dataset): \n",
    "    def __init__(self, X_data, Y_data, is_train = True):\n",
    "        self.data = torch.from_numpy(X_data).float()\n",
    "\n",
    "        self.labels = torch.from_numpy(Y_data).long()\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = BearingDataset(X_train, y_train)\n",
    "val_dataset = BearingDataset(X_test, y_test, False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle= True, num_workers= 0) \n",
    "val_loader = DataLoader(val_dataset, batch_size= BATCH_SIZE, shuffle=False, num_workers= 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
